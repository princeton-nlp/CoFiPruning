{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoFiPruning.models.l0_module import L0Module\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# a = torch.load(\"/scratch/gpfs/mengzhou/space2/out/test/CoFi_opt-125m_Books3/checkpoint-500/l0_module.pt\", map_location=torch.device('cpu'))\n",
    "b = torch.load(\"/scratch/gpfs/mengzhou/space2/out/test/CoFi_opt-125m_Books3/checkpoint-500/pytorch_model.bin\", map_location=torch.device('cpu'))\n",
    "c = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        ...,\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[\"model.decoder.layers.0.self_attn.out_proj.weight\"] == c.model.decoder.layers[0].self_attn.out_proj.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1148, -0.1440,  0.0555,  ...,  0.2147,  0.0835,  0.0668],\n",
       "        [ 0.1148, -0.1440,  0.0547,  ...,  0.2146,  0.0835,  0.0668],\n",
       "        [-0.0022, -0.0893,  0.0993,  ..., -0.0376,  0.0096, -0.1073],\n",
       "        ...,\n",
       "        [ 0.1150, -0.1439,  0.0548,  ...,  0.2151,  0.0834,  0.0670],\n",
       "        [ 0.1149, -0.1446,  0.0559,  ...,  0.2146,  0.0835,  0.0669],\n",
       "        [ 0.1146, -0.1431,  0.0553,  ...,  0.2132,  0.0836,  0.0671]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[\"model.decoder.embed_tokens.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1150, -0.1438,  0.0555,  ...,  0.2146,  0.0833,  0.0669],\n",
       "        [ 0.1149, -0.1438,  0.0547,  ...,  0.2145,  0.0833,  0.0669],\n",
       "        [ 0.0010, -0.0922,  0.1025,  ..., -0.0402,  0.0060, -0.1078],\n",
       "        ...,\n",
       "        [ 0.1152, -0.1437,  0.0547,  ...,  0.2145,  0.0833,  0.0671],\n",
       "        [ 0.1151, -0.1455,  0.0546,  ...,  0.2156,  0.0837,  0.0673],\n",
       "        [ 0.1156, -0.1437,  0.0577,  ...,  0.2139,  0.0833,  0.0650]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.model.decoder.embed_tokens.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoFiPruning.models.modeling_opt import CoFiOPTForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CoFiOPTForCausalLM.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'model.decoder.final_layer_norm.weight', 'model.decoder.final_layer_norm.bias', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.decoder.layers.6.fc1.weight', 'model.decoder.layers.6.fc1.bias', 'model.decoder.layers.6.fc2.weight', 'model.decoder.layers.6.fc2.bias', 'model.decoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.decoder.layers.7.fc1.weight', 'model.decoder.layers.7.fc1.bias', 'model.decoder.layers.7.fc2.weight', 'model.decoder.layers.7.fc2.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.weight', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.weight', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.8.fc1.weight', 'model.decoder.layers.8.fc1.bias', 'model.decoder.layers.8.fc2.weight', 'model.decoder.layers.8.fc2.bias', 'model.decoder.layers.8.final_layer_norm.weight', 'model.decoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.9.self_attn.k_proj.weight', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.9.self_attn.v_proj.weight', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.9.self_attn_layer_norm.weight', 'model.decoder.layers.9.self_attn_layer_norm.bias', 'model.decoder.layers.9.fc1.weight', 'model.decoder.layers.9.fc1.bias', 'model.decoder.layers.9.fc2.weight', 'model.decoder.layers.9.fc2.bias', 'model.decoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.9.final_layer_norm.bias', 'model.decoder.layers.10.self_attn.k_proj.weight', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.v_proj.weight', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.weight', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.10.fc1.weight', 'model.decoder.layers.10.fc1.bias', 'model.decoder.layers.10.fc2.weight', 'model.decoder.layers.10.fc2.bias', 'model.decoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.10.final_layer_norm.bias', 'model.decoder.layers.11.self_attn.k_proj.weight', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.weight', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.weight', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.11.fc1.weight', 'model.decoder.layers.11.fc1.bias', 'model.decoder.layers.11.fc2.weight', 'model.decoder.layers.11.fc2.bias', 'model.decoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.11.final_layer_norm.bias', 'lm_head.weight'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test how to can fit a 13b model to a single GPU\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-13b\")\n",
    "model.cuda().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvidia_smi\n",
    "nvidia_smi.nvmlInit()\n",
    "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "# card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n",
    "info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "print(\"Total memory:\", info.total)\n",
    "print(\"Free memory:\", info.free)\n",
    "print(\"Used memory:\", info.used)\n",
    "nvidia_smi.nvmlShutdown()\n",
    "\n",
    "# an alternative \n",
    "print(torch.cuda.memory_summary())\n",
    "\n",
    "# about the difference between torch.cuda.memory_summary() and nvidia-smi\n",
    "\"\"\"Yes, calling .half() on the model as well as the inputs would give you an estimate of how much memory would be saved in the extreme case of using float16 for all operations. Make sure to check the memory usage via torch.cuda.memory_summary(), since nvidia-smi will also show the cache not only the allocated memory.\"\"\"\n",
    "# https://discuss.pytorch.org/t/using-half-precision/94995/4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.01 0.3 0.0053 20.1754 4.748 8.5589 10.7826\n",
      "1000 0.1 0.3 0.4979 100.9221 39.0003 56.7808 66.9948\n",
      "1000 1.0 0.3 0.0609 23.8185 5.9713 10.4617 14.0719\n",
      "5000 0.01 0.3 0.3493 32.4293 11.5003 20.6394 25.4921\n",
      "5000 0.1 0.3 0.0735 22.7757 5.834 10.3339 12.9699\n",
      "5000 1.0 0.3 0.1798 26.5311 7.1342 12.2748 16.6136\n",
      "10000 0.01 0.3 0.4935 38.7686 1.0} 12.6062 1.0}\n",
      "10000 0.1 0.3 0.4914 62.5191 20.2766 33.0188 50.7191\n",
      "10000 1.0 0.3 0.2909 32.527 8.619 15.6006 21.3937\n",
      "1000 0.01 0.6 0.0053 20.1622 4.7454 8.5522 10.7975\n",
      "1000 0.1 0.6 0.5772 138.3106 62.3504 83.3525 114.8327\n",
      "1000 1.0 0.6 0.2641 43.4385 12.6153 20.7291 27.8756\n",
      "5000 0.01 0.6 0.0129 10.873 0.5} 20.645 0.5}\n",
      "5000 0.1 0.6 0.7593 146.1183 69.7352 95.0185 165.1519\n",
      "5000 1.0 0.6 0.6864 208.9983 99.2549 118.4172 237.3473\n",
      "10000 0.01 0.6 0.0 20.0163 4.6954 8.5148 10.6908\n",
      "10000 0.1 0.6 0.7847 161.7515 60.8145 88.2342 166.8606\n",
      "10000 1.0 0.6 0.6866 93.6509 34.2631 52.9171 91.4273\n",
      "1000 0.01 0.9 0.0053 20.1545 4.7376 8.5548 10.7979\n",
      "1000 0.1 0.9 0.6354 247.6805 156.8412 147.618 193.2873\n",
      "1000 1.0 0.9 0.8949 1295.5627 1160.6124 869.1015 1521.6621\n",
      "5000 0.01 0.9 0.526 58.8393 22.5546 41.3277 50.9252\n",
      "5000 0.1 0.9 0.855 353.2884 327.8697 249.1257 511.2128\n",
      "5000 1.0 0.9 0.9714 1218.1275 1235.4926 626.3773 2079.7842\n",
      "10000 0.01 0.9 0.509 57.1036 0.5} 19.2793 0.5}\n",
      "10000 0.1 0.9 0.9307 525.9727 368.0653 292.1502 850.3341\n",
      "10000 1.0 0.9 0.7034 600.5611 362.6433 312.6692 913.7368\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for ts in [0.3, 0.6, 0.9]:\n",
    "    for n in [1000, 5000, 10000]:\n",
    "        for lr in [0.01, 0.1, 1.0]:\n",
    "        \n",
    "            f = f\"/scratch/gpfs/mengzhou/space2/out/test_round1_datanum_reglr_sparsity/CoFi_{n}_{lr}_{ts}_opt-1.3b/log.txt\"\n",
    "            if not os.path.exists(f): # or not os.path.exists(os.path.dirname(f) + \"/pytorch_model.bin\"):\n",
    "                continue\n",
    "            \n",
    "            lines = open(f, \"r\").readlines()\n",
    "            num = []\n",
    "            for i, line in enumerate(lines):\n",
    "                if \"Eval @ \" in line:\n",
    "                    num.append(i) \n",
    "\n",
    "            num = num[-4]\n",
    "\n",
    "            count = 0\n",
    "            sparsity = 0\n",
    "            ppls = []\n",
    "            for line in lines[num:]:\n",
    "                # if \"sparsity:\" in line and \"target\" not in line:\n",
    "                if \"ppl\" in line:\n",
    "                    ppls.append(line.strip().split(\" \")[-1])\n",
    "                    count += 1\n",
    "                if \"sparsity\" in line and sparsity == 0:\n",
    "                    sparsity = line.strip().split(\" \")[-1]\n",
    "                if count == 4:\n",
    "                    break\n",
    "            print(f\"{n} {lr} {ts} {sparsity} {ppls[0]} {ppls[1]} {ppls[2]} {ppls[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "lr = 1.0\n",
    "ts = 0.6\n",
    "\n",
    "f = f\"/scratch/gpfs/mengzhou/space2/out/test_round1_datanum_reglr_sparsity/CoFi_{n}_{lr}_{ts}_opt-1.3b/trainer_state.json\"\n",
    "\n",
    "    \n",
    "import json\n",
    "state = json.load(open(f, \"r\"))\n",
    "sparsity = []\n",
    "ppls = []\n",
    "domain = \"Books3\"\n",
    "key = \"eval_Books3-valid-1.536M_ppl\"\n",
    "for log in state[\"log_history\"][::4]:\n",
    "    if key in log:\n",
    "        sparsity.append(log[\"sparsity\"])\n",
    "        ppls.append(log[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity every 50 steps: 0.05 0.07 0.67 0.77 0.8 0.79 0.61 0.56 0.53\n",
      "ppl every 50 steps: 22.35 22.44 294.89 302.55 309.64 297.63 119.76 85.3 68.13\n"
     ]
    }
   ],
   "source": [
    "print(\"sparsity every 50 steps:\", \" \".join(str(round(s, 2)) for s in sparsity))\n",
    "print(\"ppl every 50 steps:\", \" \".join(str(round(s, 2)) for s in ppls))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 0.1 0.3 0.3 100.9221 39.0003 56.7808 66.9948\n",
    "1000 0.1 0.6 0.6 138.3106 62.3504 83.3525 114.8327\n",
    "1000 1.0 0.3 0.3 23.8185 5.9713 10.4617 14.0719\n",
    "1000 1.0 0.6 0.6 43.4385 12.6153 20.7291 27.8756\n",
    "1000 1.0 0.9 0.9 1295.5627 1160.6124 869.1015 1521.6621"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crossd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "069398902e92ae17444dd4541ebf66b16c220c610ccf7b6a005939cdcc40384b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
